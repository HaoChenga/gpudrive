{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-playback (expert) trajectory extraction and usage\n",
    "\n",
    "This notebook demonstrates how to extract expert actions using different dynamics models and step through scene with those actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import mediapy\n",
    "\n",
    "# Set working directory to the base directory 'gpudrive'\n",
    "working_dir = Path.cwd()\n",
    "while working_dir.name != 'gpudrive':\n",
    "    working_dir = working_dir.parent\n",
    "    if working_dir == Path.home():\n",
    "        raise FileNotFoundError(\"Base directory 'gpudrive' not found\")\n",
    "os.chdir(working_dir)\n",
    "\n",
    "from pygpudrive.env.config import EnvConfig, RenderConfig, SceneConfig\n",
    "from pygpudrive.env.env_torch import GPUDriveTorchEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DYNAMICS_MODEL = \"state\" # \"delta_local\" or \"state\"\n",
    "# DATA_PATH = \"data/examples\" # Your data path\n",
    "DATA_PATH =\"/mnt/nas_9/group/penghaocheng/code/multi_agent_with_LLM/dataset/output/visualization_data/test_small_scenes.txt\"\n",
    "MAX_NUM_OBJECTS = 128\n",
    "NUM_ENVS = 10\n",
    "\n",
    "# Configs\n",
    "render_config = RenderConfig(draw_obj_idx=True)\n",
    "scene_config = SceneConfig(path=DATA_PATH, num_scenes=NUM_ENVS)\n",
    "env_config = EnvConfig(dynamics_model=DYNAMICS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_11/tfrecord-00829-of-01000_101.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_11/tfrecord-00879-of-01000_397.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_11/tfrecord-00931-of-01000_238.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_11/tfrecord-00951-of-01000_181.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00079-of-01000_352.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00105-of-01000_214.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00129-of-01000_299.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00129-of-01000_69.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00228-of-01000_266.json', '/mnt/nas_9/group/penghaocheng/code/gpudrive/dataset/dataset/formatted_json_v2_no_tl_train_split_1000/batch_4/tfrecord-00229-of-01000_142.json']\n"
     ]
    }
   ],
   "source": [
    "env = GPUDriveTorchEnv(\n",
    "    config=env_config,\n",
    "    scene_config=scene_config,\n",
    "    max_cont_agents=MAX_NUM_OBJECTS,\n",
    "    device=\"cpu\",\n",
    "    render_config=render_config,\n",
    "    action_type=\"continuous\" # \"continuous\" or \"discrete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log-playback (expert) actions\n",
    "\n",
    "- Different dynamics models have different action spaces. For details, [see the docs.](https://github.com/Emerge-Lab/gpudrive/tree/main/pygpudrive/env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 91, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract full expert trajectory\n",
    "expert_actions, _, _ = env.get_expert_actions()\n",
    "\n",
    "expert_actions.shape # Shape: (num_envs, num_steps, num_agents, num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step through an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "obs = env.reset()\n",
    "import pickle\n",
    "import numpy as np\n",
    "mfile='/mnt/nas_9/group/penghaocheng/code/multi_agent_with_LLM/dataset/output/visualization_data/reserve_agent_mask_for_test_small.pkl'\n",
    "with open(mfile,\"rb\") as m:\n",
    "    mask=pickle.load(m)\n",
    "\n",
    "frames = {f\"env_{i}\": [] for i in range(NUM_ENVS)}\n",
    "path='/mnt/nas_9/group/penghaocheng/code/multi_agent_with_LLM/dataset/output/visualization_data/pred.pkl'\n",
    "with open(path,\"rb\") as f:\n",
    "    pred_trajs=pickle.load(f)\n",
    "\n",
    "pred_trajs=np.transpose(pred_trajs,(0,2,1,3,4))\n",
    "# print(pred_trajs.shape)#10,10,90,7,2\n",
    "# Step through the scene\n",
    "for t in range(env_config.episode_len):\n",
    "    env.step_dynamics(expert_actions[:, :, t, :])\n",
    "    \n",
    "    # Render the scenes\n",
    "    for i in range(NUM_ENVS):\n",
    "        if i in [0,5,9]:\n",
    "            frames[f\"env_{i}\"].append(env.render(i,t,pred_trajs[i],mask[i]))\n",
    "        # if i ==0:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show resulting trajectories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show videos\n",
    "import cv2\n",
    "# os.makedirs('./videos',exist_ok=True)\n",
    "\n",
    "\n",
    "for i in [0,5,9]:\n",
    "    image_list=frames[f'env_{i}']\n",
    "    os.makedirs(f'./results/env_{i}/',exist_ok=True)\n",
    "    for j in range(0,len(image_list),9):\n",
    "        cv2.imwrite(f'./results/env_{i}/{j}.jpg',image_list[j])\n",
    "    # output_video = f\"./videos/env_{i}.mp4\"  # 输出视频文件名\n",
    "    # fps = 10  # 设置帧率\n",
    "\n",
    "    # # 获取图像尺寸\n",
    "    # height, width, layers = image_list[0].shape\n",
    "\n",
    "    # # 定义 VideoWriter，使用 mp4v 编码\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # 适用于 .mp4\n",
    "    # video = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    # # 逐帧写入视频\n",
    "    # for frame in image_list:\n",
    "    #     video.write(frame)  # frame 已经是 NumPy 数组，不需要再读取\n",
    "\n",
    "    # # 释放资源\n",
    "    # video.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    # print(\"MP4 视频已保存:\", output_video)\n",
    "    # mediapy.show_videos(frames, fps=30, width=600, height=400, columns=2,codec='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpudrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
